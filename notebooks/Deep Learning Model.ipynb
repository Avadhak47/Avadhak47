{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avadhak47/Avadhak47/blob/main/notebooks/Deep%20Learning%20Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FHMn9DKy6KR"
      },
      "source": [
        "Human Mood recognition model\n",
        "\n",
        "* dataset : fer2013\n",
        "* classifier: EDNN model for FER by Deepak Kumar Jaina, Pourya Shamsolmoalib &\n",
        "    Paramjit Sehdev, as it appears in \"Extended deep neural network for \n",
        "    facial emotion recognition\", 2019 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuteZy-x2V53"
      },
      "outputs": [],
      "source": [
        "# installing tensorflow\n",
        "!pip install -q tensorflow==2.0.0 tensorboard==2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1lBFXpa1394"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UU7cY7wL139a"
      },
      "source": [
        "## The dataset loader\n",
        "\n",
        "This function will allow us to load directly all the dataset using pandas. This will also split the dataset into training and testing subsets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL-JB1oz139d"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def load_dataset(net=True):\n",
        "    \"\"\"Utility function to load the FER2013 dataset.\n",
        "    \n",
        "    It returns the formated tuples (X_train, y_train) , (X_test, y_test).\n",
        "    \"\"\"\n",
        "\n",
        "    # Load and filter in Training/not Training data:\n",
        "    df = pd.read_csv('../data/fer2013.csv')\n",
        "    training = df.loc[df['Usage'] == 'Training']\n",
        "    testing = df.loc[df['Usage'] != 'Training']\n",
        "\n",
        "    # X_train values:\n",
        "    X_train = training[['pixels']].values\n",
        "    X_train = [np.fromstring(e[0], dtype=int, sep=' ') for e in X_train]\n",
        "\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ==========\n",
        "    net : boolean\n",
        "        This parameter is used to reshape the data from images in \n",
        "        (cols, rows, channels) format. In case that it is False, a standard\n",
        "        format (cols, rows) is used.\n",
        "    \"\"\"\n",
        "    if net:\n",
        "        X_train = [e.reshape((48, 48, 1)).astype('float32') for e in X_train]\n",
        "    else:\n",
        "        X_train = [e.reshape((48, 48)) for e in X_train]\n",
        "    X_train = np.array(X_train)\n",
        "\n",
        "    # X_test values:\n",
        "    X_test = testing[['pixels']].values\n",
        "    X_test = [np.fromstring(e[0], dtype=int, sep=' ') for e in X_test]\n",
        "    if net:\n",
        "        X_test = [e.reshape((48, 48, 1)).astype('float32') for e in X_test]\n",
        "    else:\n",
        "        X_test = [e.reshape((48, 48)) for e in X_test]\n",
        "    X_test = np.array(X_test)\n",
        "\n",
        "    # y_train values:\n",
        "    y_train = training[['emotion']].values\n",
        "    y_train = keras.utils.to_categorical(y_train)\n",
        "\n",
        "    # y_test values\n",
        "    y_test = testing[['emotion']].values\n",
        "    y_test = keras.utils.to_categorical(y_test)\n",
        "\n",
        "    return (X_train, y_train) , (X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GooWPAho139m"
      },
      "source": [
        "We can now use our main function to load the complete dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxjpLkjZ139n"
      },
      "outputs": [],
      "source": [
        "(X_train, y_train) , (X_test, y_test) = load_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj-e6Ew4139r"
      },
      "source": [
        "We can access the data and plot some samples to check out waht's inside:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v3fuYQb139s"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "plt.figure(figsize=(12, 12))\n",
        "for i in range(16):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(X_train[i].reshape((48, 48)), cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3gjIRnx139x"
      },
      "source": [
        "And, to understand how each image is loaded, we can print any element as a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEgSq4CR139z"
      },
      "outputs": [],
      "source": [
        "X_train[i].reshape((48, 48))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1TUPR6t1398"
      },
      "source": [
        "We can now proceed to build our EDNN model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmjgIGFM1399"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "\n",
        "def ResidualBlock(prev_layer):\n",
        "    \"\"\"Residual block from the EDNN model for FER by Deepak Kumar Jaina,\n",
        "    Pourya Shamsolmoalib & Paramjit Sehdev, as it appears in \"Extended \n",
        "    deep neural network for facial emotion recognition\", 2019.\n",
        "    \"\"\"\n",
        "    conv_1 = Conv2D(64, (1, 1))(prev_layer)\n",
        "    conv_2 = Conv2D(64, (3, 3), padding=\"same\")(conv_1)\n",
        "    shortc = concatenate([conv_1, conv_2], axis=-1)\n",
        "    conv_3 = Conv2D(128, (3, 3), padding=\"same\")(shortc)\n",
        "    conv_4 = Conv2D(256, (1, 1))(conv_3)\n",
        "    output = concatenate([conv_4, prev_layer], axis=-1)\n",
        "    \n",
        "    return output\n",
        "\n",
        "\n",
        "def EDNN(n_classes=7):\n",
        "    \"\"\"\n",
        "    EDNN model for FER by Deepak Kumar Jaina, Pourya Shamsolmoalib &\n",
        "    Paramjit Sehdev, as it appears in \"Extended deep neural network for \n",
        "    facial emotion recognition\", 2019.\n",
        "    \"\"\"\n",
        "\n",
        "    x = Input(shape=(48, 48, 1))\n",
        "    y = Conv2D(32, (5, 5), input_shape=(48, 48, 1), strides=(2, 2), \n",
        "               data_format='channels_last')(x)\n",
        "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "    y = Conv2D(64, (3, 3), strides=(1, 1))(y)\n",
        "    y = ResidualBlock(y)\n",
        "    y = Conv2D(128, (3, 3), strides=(1, 1), padding=\"same\")(y)\n",
        "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "    y = Conv2D(128, (3, 3), strides=(1, 1))(y)\n",
        "    y = ResidualBlock(y)\n",
        "    y = Conv2D(256, (3, 3), strides=(1, 1), padding=\"same\")(y)\n",
        "    y = MaxPooling2D(pool_size=(2, 2))(y)\n",
        "    y = Conv2D(512, (3, 3), strides=(1, 1), padding=\"same\")(y)\n",
        "    y = Flatten()(y)\n",
        "    y = Dense(1024, activation='relu')(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Dense(512, activation='relu')(y)\n",
        "    y = Dropout(0.2)(y)\n",
        "    y = Dense(n_classes, activation='softmax')(y)\n",
        "    \n",
        "    # Create model:\n",
        "    model = Model(x, y)\n",
        "\n",
        "    # Compile model:\n",
        "    opt = SGD(lr=LRATE, momentum=0.9, decay=LRATE/EPOCHS)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiJlZBRt13-B"
      },
      "source": [
        "We now create an instance of our model and verify the details of the architecture:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krUkgf_R13-C"
      },
      "outputs": [],
      "source": [
        "# Set hyperparameters:\n",
        "EPOCHS = 1\n",
        "BATCH = 64\n",
        "LRATE = 1e-4\n",
        "\n",
        "# Instance model\n",
        "ednn = EDNN()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQPAEfGo13-H"
      },
      "outputs": [],
      "source": [
        "ednn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpT6c7Ht13-L"
      },
      "source": [
        "## Model training\n",
        "\n",
        "So far we have created out model and we already have loaded the datset. But beofr, let's create our media folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BXz14wy77HS"
      },
      "outputs": [],
      "source": [
        "!mkdir ../media"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZ8iD3QM7n3b"
      },
      "source": [
        "We can now proceed to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJDgVy9r13-M"
      },
      "outputs": [],
      "source": [
        "history = ednn.fit(X_train, y_train,\n",
        "                   validation_data=(X_test, y_test),\n",
        "                   epochs=EPOCHS, batch_size=BATCH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPqu0fhd13-P"
      },
      "source": [
        "After the training, we can plot the history of this process.\n",
        "\n",
        "The following functions will allow us to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1bG8vgi13-R"
      },
      "outputs": [],
      "source": [
        "def plot_loss(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title(\"Model's training loss\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_accuracy(history):\n",
        "    plt.style.use(\"ggplot\")\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title(\"Model's training accuracy\")\n",
        "    plt.xlabel(\"Epoch #\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_WO5WLA13-V"
      },
      "outputs": [],
      "source": [
        "# Plot loss:\n",
        "plot_loss(history)\n",
        "# plt.savefig('../media/loss.png', dpi=300)\n",
        "\n",
        "# Plot accuracy:\n",
        "plot_accuracy(history)\n",
        "# plt.savefig('../media/accuracy.png', dpi=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVCjGBb08yPp"
      },
      "source": [
        "A way to explore accuracy through classes is using a confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_5pIt3m13-Y"
      },
      "outputs": [],
      "source": [
        "# Create emotions map:\n",
        "emotion_labels = [\n",
        "    'Angry',\n",
        "    'Disgust',\n",
        "    'Fear',\n",
        "    'Happy',\n",
        "    'Sad',\n",
        "    'Surprise',\n",
        "    'Neutral'\n",
        "]\n",
        "\n",
        "# Predict using trained model:\n",
        "y_pred = ednn.predict(X_test)\n",
        "y_pred = np.asarray([np.argmax(e) for e in y_pred])\n",
        "y_true = np.asarray([np.argmax(e) for e in y_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Ztasu0w84B_"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Compute confusion matrix:\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "cm_normalised = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "# Plot confusion matrix:\n",
        "sns.set(font_scale=1.5) \n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax = sns.heatmap(cm_normalised, annot=True, linewidths=0, square=False, \n",
        "                 cmap='gray', yticklabels=emotion_labels,\n",
        "                 xticklabels=emotion_labels, vmin=0,\n",
        "                 vmax=np.max(cm_normalised), fmt=\".2f\",\n",
        "                 annot_kws={\"size\": 20})\n",
        "ax.set(xlabel='Predicted label', ylabel='True label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu1sXL019CLN"
      },
      "source": [
        "## Saving the trained model\n",
        "\n",
        "To save the trained model we will basically do two things:\n",
        "\n",
        "- Serialize the model into a JSON file, which will save the architecture of our model.\n",
        "- Serialize the weights into a HDF5 file, which will save all parameters of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xy0H-d419DeD"
      },
      "outputs": [],
      "source": [
        "# Serialize model to JSON:\n",
        "json_ednn = ednn.to_json()\n",
        "with open('../data/model.json', 'w') as json_file:\n",
        "    json_file.write(json_ednn)\n",
        "\n",
        "# Serialize weights to HDF5 (h5py needed):\n",
        "ednn.save_weights('../data/model.h5')\n",
        "print('Model saved to disk.')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "Deep Learning Model",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
